{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:19.337147Z","iopub.status.busy":"2023-11-13T22:30:19.336301Z","iopub.status.idle":"2023-11-13T22:30:19.351617Z","shell.execute_reply":"2023-11-13T22:30:19.350253Z","shell.execute_reply.started":"2023-11-13T22:30:19.337101Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in\n","\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import lr_scheduler\n","from tqdm import tqdm\n","import torch.nn as nn\n","import math\n","import time\n","import torch\n","from numpy import array\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{},"source":["Lets load the CSV file into a Pandas dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T09:39:09.239361Z","iopub.status.busy":"2023-11-14T09:39:09.238971Z","iopub.status.idle":"2023-11-14T09:39:09.657707Z","shell.execute_reply":"2023-11-14T09:39:09.656089Z","shell.execute_reply.started":"2023-11-14T09:39:09.239330Z"},"trusted":true},"outputs":[],"source":["# data = pd.read_csv('/kaggle/input/data-csv/data.csv', dtype=str)\n","# data = pd.read_csv('/Users/fernando/PyNetSim/tutorials/surrogate/data/data.csv', dtype=str)\n","data = pd.read_csv(\n","    '/Users/ffjla/PyNetSim/tutorials/surrogate/cluster_heads/data/data.csv', dtype=str)"]},{"cell_type":"markdown","metadata":{},"source":["Lets check the shape of the dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.783362Z","iopub.status.busy":"2023-11-13T22:30:22.782931Z","iopub.status.idle":"2023-11-13T22:30:22.790238Z","shell.execute_reply":"2023-11-13T22:30:22.788972Z","shell.execute_reply.started":"2023-11-13T22:30:22.783327Z"},"trusted":true},"outputs":[],"source":["print(data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Lets get only the first 10% of the data to speed up the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.792492Z","iopub.status.busy":"2023-11-13T22:30:22.792006Z","iopub.status.idle":"2023-11-13T22:30:22.863331Z","shell.execute_reply":"2023-11-13T22:30:22.861901Z","shell.execute_reply.started":"2023-11-13T22:30:22.792456Z"},"trusted":true},"outputs":[],"source":["# print data set info\n","print(data.info())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dataframe to hold mean and std of each column\n","data_stats = pd.DataFrame(columns=['name', 'mean', 'std'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the mean and std of each column\n","name_values = data['name']\n","name_values = name_values.apply(lambda x: eval(x))\n","alpha_values = name_values.apply(lambda x: x[0])\n","beta_values = name_values.apply(lambda x: x[1])\n","gamma_values = name_values.apply(lambda x: x[2])\n","name_stats_dict = {\n","    'alpha_mean': alpha_values.mean(),\n","    'alpha_std': alpha_values.std(),\n","    'beta_mean': beta_values.mean(),\n","    'beta_std': beta_values.std(),\n","    'gamma_mean': gamma_values.mean(),\n","    'gamma_std': gamma_values.std()\n","}\n","# Lets add to the dataframe\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'alpha', 'mean': alpha_values.mean(), 'std': alpha_values.std()}, index=[0])])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'beta', 'mean': beta_values.mean(), 'std': beta_values.std()}, index=[0])])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'gamma', 'mean': gamma_values.mean(), 'std': gamma_values.std()}, index=[0])])\n","# reset index\n","data_stats = data_stats.reset_index(drop=True)\n","print(data_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_stats(name, data):\n","    data = data.apply(lambda x: eval(x))\n","    data_mean = data.mean()\n","    data_std = data.std()\n","    data_stats_dict = {\n","        'name': name,\n","        'mean': data_mean,\n","        'std': data_std\n","    }\n","    return data_stats_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_array_stats(name, data):\n","    data = data.apply(lambda x: eval(x))\n","    data_mean = data.apply(lambda x: np.mean(x)).mean()\n","    data_std = data.apply(lambda x: np.std(x)).mean()\n","    data_stats_dict = {\n","        'name': name,\n","        'mean': data_mean,\n","        'std': data_std\n","    }\n","    return data_stats_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the remaining_energy column\n","remaining_energy_stats_dict = compute_stats('re',\n","                                            data['remaining_energy'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    remaining_energy_stats_dict, index=[0])]).reset_index(drop=True)\n","chs_dict = compute_array_stats('chs', data['cluster_heads'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    chs_dict, index=[0])]).reset_index(drop=True)\n","el_dict = compute_array_stats('el', data['energy_levels'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    el_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_ch_to_sink_dict = compute_array_stats(\n","    'energy_dissipated_ch_to_sink', data['energy_dissipated_ch_to_sink'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_ch_to_sink_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_non_ch_to_ch_dict = compute_array_stats(\n","    'energy_dissipated_non_ch_to_ch', data['energy_dissipated_non_ch_to_ch'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_non_ch_to_ch_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_ch_rx_from_non_ch_dict = compute_array_stats(\n","    'energy_dissipated_ch_rx_from_non_ch', data['energy_dissipated_ch_rx_from_non_ch'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_ch_rx_from_non_ch_dict, index=[0])]).reset_index(drop=True)\n","print(data_stats.to_string())"]},{"cell_type":"markdown","metadata":{},"source":["Proportion of the data that will be used for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Standardize data using F1 score\n","def standardize_inputs(x, mean, std):\n","    standardized_x = (x - mean) / std\n","    return standardized_x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.899390Z","iopub.status.busy":"2023-11-13T22:30:22.898998Z","iopub.status.idle":"2023-11-13T22:31:33.422421Z","shell.execute_reply":"2023-11-13T22:31:33.421109Z","shell.execute_reply.started":"2023-11-13T22:30:22.899356Z"},"trusted":true},"outputs":[],"source":["def split_sequence(sequence, n_steps):\n","    global non_cyclical_features_size, cyclical_features_size\n","    x_data = []\n","    y_data = []\n","    num_samples = len(sequence)\n","\n","    for i in tqdm(range(num_samples), desc=\"Processing sequence\"):\n","        end_ix = i + n_steps\n","        if end_ix > num_samples - 1:\n","            break\n","        # Get the alpha, beta, gamma\n","        name = sequence['name'][i]\n","        name = eval(name)\n","        alpha = name[0]\n","        beta = name[1]\n","        gamma = name[2]\n","\n","        # Last alpha, beta, gamma\n","        last_name = sequence['name'][end_ix]\n","        last_name = eval(last_name)\n","        last_alpha = last_name[0]\n","        last_beta = last_name[1]\n","        last_gamma = last_name[2]\n","\n","        if alpha != last_alpha or beta != last_beta or gamma != last_gamma:\n","            continue\n","\n","        alpha = standardize_inputs(alpha, data_stats.loc[data_stats['name'] == 'alpha']['mean'].values[0],\n","                                   data_stats.loc[data_stats['name'] == 'alpha']['std'].values[0])\n","        beta = standardize_inputs(beta, data_stats.loc[data_stats['name'] == 'beta']['mean'].values[0],\n","                                  data_stats.loc[data_stats['name'] == 'beta']['std'].values[0])\n","        gamma = standardize_inputs(gamma, data_stats.loc[data_stats['name'] == 'gamma']['mean'].values[0],\n","                                   data_stats.loc[data_stats['name'] == 'gamma']['std'].values[0])\n","        # print(f\"alpha: {alpha}, beta: {beta}, gamma: {gamma}\")\n","        if n_steps <= 1:\n","            remaining_energy = sequence['remaining_energy'][i]\n","            remaining_energy = eval(remaining_energy)\n","            remaining_energy = [float(remaining_energy)]\n","            pchs = sequence['potential_cluster_heads'][i]\n","            pchs = eval(pchs)\n","            pchs = [[int(x) for x in pchs]]\n","            energy_levels = sequence['energy_levels'][i]\n","            energy_levels = eval(energy_levels)\n","            energy_levels = [[float(x) for x in energy_levels]]\n","            energy_dissipated_ch_to_sink = sequence['energy_dissipated_ch_to_sink'][i]\n","            energy_dissipated_ch_to_sink = eval(energy_dissipated_ch_to_sink)\n","            energy_dissipated_ch_to_sink = [\n","                [float(x) for x in energy_dissipated_ch_to_sink]]\n","            energy_dissipated_non_ch_to_ch = sequence['energy_dissipated_non_ch_to_ch'][i]\n","            energy_dissipated_non_ch_to_ch = eval(\n","                energy_dissipated_non_ch_to_ch)\n","            energy_dissipated_non_ch_to_ch = [\n","                [float(x) for x in energy_dissipated_non_ch_to_ch]]\n","            energy_dissipated_ch_rx_from_non_ch = sequence['energy_dissipated_ch_rx_from_non_ch'][i]\n","            energy_dissipated_ch_rx_from_non_ch = eval(\n","                energy_dissipated_ch_rx_from_non_ch)\n","            energy_dissipated_ch_rx_from_non_ch = [\n","                [float(x) for x in energy_dissipated_ch_rx_from_non_ch]]\n","            num_cluster_heads = sequence['alive_nodes'][i]\n","            num_cluster_heads = eval(num_cluster_heads)\n","            # lets round up to the nearest integer\n","            num_cluster_heads = [(int(num_cluster_heads*0.05)+1)/5]\n","            cluster_heads = sequence['cluster_heads'][i]\n","            cluster_heads = eval(cluster_heads)\n","            cluster_heads = [int(x) for x in cluster_heads]\n","        else:\n","            raise f\"n_steps: {n_steps} not supported\"\n","        # print(f\"remaining_energy: {remaining_energy}\")\n","        # print(f\"pchs: {pchs}\")\n","        # print(f\"energy_levels: {energy_levels}\")\n","        # print(\n","        #     f\"energy_dissipated_ch_to_sink: {energy_dissipated_ch_to_sink}\")\n","        # print(\n","        #     f\"energy_dissipated_non_ch_to_ch: {energy_dissipated_non_ch_to_ch}\")\n","        # print(\n","        #     f\"energy_dissipated_ch_rx_from_non_ch: {energy_dissipated_ch_rx_from_non_ch}\")\n","        # print(f\"num_cluster_heads: {num_cluster_heads}\")\n","        # print(f\"cluster_heads: {cluster_heads}\")\n","        # Standardize remaining energy\n","        remaining_energy = standardize_inputs(\n","            remaining_energy, data_stats.loc[data_stats['name']\n","                                             == 're']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 're']['std'].values[0])\n","\n","        seq_y = sequence['cluster_heads_index'][end_ix-1]\n","        seq_y = eval(seq_y)\n","        seq_y = [int(x) for x in seq_y]\n","\n","        energy_levels = standardize_inputs(\n","            energy_levels, data_stats.loc[data_stats['name']\n","                                          == 'el']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 'el']['std'].values[0])\n","\n","        energy_dissipated_ch_to_sink = standardize_inputs(\n","            energy_dissipated_ch_to_sink, data_stats.loc[data_stats['name']\n","                                                         == 'energy_dissipated_ch_to_sink']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_ch_to_sink']['std'].values[0])\n","\n","        energy_dissipated_non_ch_to_ch = standardize_inputs(\n","            energy_dissipated_non_ch_to_ch, data_stats.loc[data_stats['name']\n","                                                           == 'energy_dissipated_non_ch_to_ch']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_non_ch_to_ch']['std'].values[0])\n","\n","        energy_dissipated_ch_rx_from_non_ch = standardize_inputs(\n","            energy_dissipated_ch_rx_from_non_ch, data_stats.loc[data_stats['name']\n","                                                                == 'energy_dissipated_ch_rx_from_non_ch']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_ch_rx_from_non_ch']['std'].values[0])\n","\n","        cluster_heads = standardize_inputs(\n","            cluster_heads, data_stats.loc[data_stats['name']\n","                                          == 'chs']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 'chs']['std'].values[0])\n","        # print(f\"cluster_heads: {cluster_heads}\")\n","        seq_x = []\n","        if n_steps <= 1:\n","            seq_x.extend([alpha, beta, gamma])\n","            seq_x.extend(remaining_energy)\n","            seq_x.extend(pchs[0])\n","            seq_x.extend(energy_levels[0])\n","            seq_x.extend(energy_dissipated_ch_to_sink[0])\n","            # seq_x.extend(energy_dissipated_non_ch_to_ch[0])\n","            seq_x.extend(energy_dissipated_ch_rx_from_non_ch[0])\n","            seq_x.extend(num_cluster_heads)\n","        else:\n","            raise f\"n_steps: {n_steps} not supported\"\n","        # print(f\"seq_x: {seq_x}\")\n","        # print(f\"len(seq_x): {len(seq_x)}\")\n","        # print(f\"seq_y: {seq_y}\")\n","        # return\n","\n","        x_data.append(seq_x)\n","        y_data.append(seq_y)\n","\n","    return np.array(x_data), np.array(y_data)\n","\n","\n","n_steps = 1\n","x_data, y_data = split_sequence(data, n_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print the array where the first three rows are 0.8485311004611152, 0.6836234887293634, -0.4393387688162076\n","needed_data = np.where(x_data[:, 0] == 0.8485311004611152)\n","# print the entire row\n","print(list(x_data[needed_data]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np_x = np.array(x_data)\n","print(f\"np_x.shape: {np_x.shape}\")\n","np_y = np.array(y_data)\n","print(np_y.shape)\n","# np_y = np.argmax(np_y, axis=2)\n","# print(np_y.shape)\n","# print(np_y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into train and test\n","x_train, x_test, y_train, y_test = train_test_split(\n","    np_x, np_y, test_size=0.1, random_state=42, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset class."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.432915Z","iopub.status.busy":"2023-11-13T22:31:33.432564Z","iopub.status.idle":"2023-11-13T22:31:33.446920Z","shell.execute_reply":"2023-11-13T22:31:33.445701Z","shell.execute_reply.started":"2023-11-13T22:31:33.432885Z"},"trusted":true},"outputs":[],"source":["class ClusterHeadDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.X = torch.from_numpy(x.astype(np.float32))\n","        self.y = torch.from_numpy(y.astype(np.float32))\n","        self.len = x.shape[0]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","    # Support batching\n","    def collate_fn(self, batch):\n","        X = torch.stack([x[0] for x in batch])\n","        y = torch.stack([x[1] for x in batch])\n","        return X, y"]},{"cell_type":"markdown","metadata":{},"source":["Create the network architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.450481Z","iopub.status.busy":"2023-11-13T22:31:33.449246Z","iopub.status.idle":"2023-11-13T22:31:33.465721Z","shell.execute_reply":"2023-11-13T22:31:33.464591Z","shell.execute_reply.started":"2023-11-13T22:31:33.450418Z"},"trusted":true},"outputs":[],"source":["class ForecastCCH(nn.Module):\n","    global non_cyclical_features_size, cyclical_features_size\n","\n","    def __init__(self, input_size=10, h1=100, h2=100, output_size=101):\n","        super(ForecastCCH, self).__init__()\n","        self.batch_norm1 = nn.BatchNorm1d(input_size)\n","        self.fc1 = nn.Linear(input_size, h1)\n","        self.batch_norm2 = nn.BatchNorm1d(h1)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.3)\n","\n","        self.fc2 = nn.Linear(h1, h2)\n","        self.batch_norm3 = nn.BatchNorm1d(h2)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.4)\n","\n","        self.fc3 = nn.Linear(h2, output_size)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # print(f\"x shape: {x.shape}\")\n","        out = self.batch_norm1(x)\n","        out = self.fc1(x)\n","        out = self.batch_norm2(out)\n","        out = self.relu1(out)\n","        out = self.dropout1(out)\n","        # print(f\"out shape1: {out.shape}\")\n","\n","        out = self.fc2(out)\n","        out = self.batch_norm3(out)\n","        out = self.relu2(out)\n","        out = self.dropout2(out)\n","        # print(f\"out shape2: {out.shape}\")\n","\n","        out = self.fc3(out)\n","        out = self.sigmoid(out)\n","        # print(f\"out shape3: {out.shape}\")\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.468837Z","iopub.status.busy":"2023-11-13T22:31:33.467832Z","iopub.status.idle":"2023-11-13T22:31:33.508796Z","shell.execute_reply":"2023-11-13T22:31:33.507729Z","shell.execute_reply.started":"2023-11-13T22:31:33.468774Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = ForecastCCH(\n","    input_size=401, h1=2000, h2=2000, output_size=101).to(device)\n","# # If there is a model saved, load it\n","if os.path.isfile('ch_model.pt'):\n","    model.load_state_dict(torch.load('ch_model.pt'))\n","    print(\"Model loaded\")\n","else:\n","    print(\"No model found\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)\n","criterion = nn.BCELoss()\n","rl_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset objects."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.511959Z","iopub.status.busy":"2023-11-13T22:31:33.511069Z","iopub.status.idle":"2023-11-13T22:31:33.588626Z","shell.execute_reply":"2023-11-13T22:31:33.587611Z","shell.execute_reply.started":"2023-11-13T22:31:33.511919Z"},"trusted":true},"outputs":[],"source":["train = ClusterHeadDataset(x_train, y_train)\n","valid = ClusterHeadDataset(x_test, y_test)\n","train_loader = DataLoader(train, batch_size=16, shuffle=True)\n","valid_loader = DataLoader(valid, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_accuracy(y_true, y_prob, print_info=False):\n","    assert y_true.ndim == 1 and y_true.size() == y_prob.size()\n","    y_prob = y_prob > 0.5\n","    if print_info:\n","        print(f\"y_true: {np.where(y_true == 1)}\")\n","        print(f\"y_prob: {np.where(y_prob == 1)}\")\n","        # delay 1 second\n","        time.sleep(1)\n","    return (y_true == y_prob).sum().item() / y_true.size(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_predicted(print_info=False):\n","    model.eval()\n","    avg_accuracy = []\n","    losses = []\n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(valid_loader):\n","            # print(f\"inputs: {inputs}, shape: {inputs.shape}\")\n","            # print(f\"labels: {labels}, shape: {labels.shape}\")\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            # print(f\"labels: {labels}, shape: {labels.shape}\")\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            # print(f\"preds: {preds}, shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            losses.append(loss.item())\n","            temp_accuracy = []\n","            # Loop over both the predictions and the labels\n","            for pred, label in zip(preds, labels):\n","                # print(f\"pred: {pred}, shape: {pred.shape}\")\n","                # print(f\"label: {label}, shape: {label.shape}\")\n","                accuracy = get_accuracy(label, pred, print_info)\n","                temp_accuracy.append(accuracy*100)\n","            avg_accuracy.append(np.mean(temp_accuracy))\n","    print(\n","        f\"Average loss: {np.mean(losses)}\")\n","    print(\n","        f\"Average accuracy: {np.mean(avg_accuracy)}%\")\n","    print(\n","        f\"Min Accuracy: {np.min(avg_accuracy)}%\")\n","    # number of samples with the lowest accuracy\n","    min_accuracy = np.min(avg_accuracy)\n","    min_accuracy_count = np.count_nonzero(avg_accuracy == min_accuracy)\n","    print(\n","        f\"Min Accuracy count: {min_accuracy_count}\")\n","    print(\n","        f\"Max Accuracy: {np.max(avg_accuracy)}%\")\n","    # number of samples with the highest accuracy\n","    max_accuracy = np.max(avg_accuracy)\n","    max_accuracy_count = np.count_nonzero(avg_accuracy == max_accuracy)\n","    print(\n","        f\"Max Accuracy count: {max_accuracy_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test(epoch, epochs, best_loss):\n","    running_loss = .0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(valid_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            # print(f\"test inputs shape: {inputs.shape}\")\n","            # print(f\"test labels shape: {labels.shape}\")\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            preds = preds.squeeze()\n","            # print(f\"test preds shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            running_loss += loss\n","\n","        valid_loss = running_loss/len(valid_loader)\n","        # print(f'valid_loss {valid_loss}')\n","\n","        if valid_loss < best_loss:\n","            print(\n","                f\"Epoch [{epoch}/{epochs}] Validation Loss Improved: {best_loss:.4f} -> {valid_loss:.4f}\")\n","            best_loss = valid_loss\n","            torch.save(model.state_dict(), 'ch_model.pt')\n","\n","    return best_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.592841Z","iopub.status.busy":"2023-11-13T22:31:33.592324Z","iopub.status.idle":"2023-11-13T22:31:33.605727Z","shell.execute_reply":"2023-11-13T22:31:33.604423Z","shell.execute_reply.started":"2023-11-13T22:31:33.592795Z"},"trusted":true},"outputs":[],"source":["\n","\n","def train():\n","    train_losses = []\n","    best_loss = float('inf')\n","    epochs = 1000\n","    for epoch in range(epochs):\n","        print(\n","            f'epochs {epoch}/{epochs}, LR: {optimizer.param_groups[0][\"lr\"]}')\n","        model.train()\n","        training_loss = .0\n","        # Wrap the data loader with tqdm to add a progress bar\n","        for idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n","            # print(f\"inputs shape: {inputs.shape}\")\n","            # print(f\"labels shape: {labels.shape}\")\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            # preds = preds.squeeze()\n","            # print(f\"predes shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss\n","\n","        train_loss = training_loss / len(train_loader)\n","        train_losses.append(train_loss.detach().numpy())\n","        print(f'train_loss {train_loss}')\n","        rl_scheduler.step()\n","\n","        if epoch % 5 == 0:\n","            best_loss = test(epoch, epochs, best_loss)\n","\n","        if epoch % 5 == 0:\n","            test_predicted()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.627847Z","iopub.status.busy":"2023-11-13T22:31:33.627155Z"},"trusted":true},"outputs":[],"source":["\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["valid_loader = DataLoader(valid, batch_size=1, shuffle=True)\n","test_predicted(True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3998218,"sourceId":6960150,"sourceType":"datasetVersion"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
