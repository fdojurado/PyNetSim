{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:19.337147Z","iopub.status.busy":"2023-11-13T22:30:19.336301Z","iopub.status.idle":"2023-11-13T22:30:19.351617Z","shell.execute_reply":"2023-11-13T22:30:19.350253Z","shell.execute_reply.started":"2023-11-13T22:30:19.337101Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in\n","\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import confusion_matrix\n","from torch.optim import lr_scheduler\n","from tqdm import tqdm\n","import torch.nn as nn\n","!pip install seaborn --upgrade\n","import seaborn as sns\n","import time\n","import torch\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{},"source":["Lets load the CSV file into a Pandas dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T09:39:09.239361Z","iopub.status.busy":"2023-11-14T09:39:09.238971Z","iopub.status.idle":"2023-11-14T09:39:09.657707Z","shell.execute_reply":"2023-11-14T09:39:09.656089Z","shell.execute_reply.started":"2023-11-14T09:39:09.239330Z"},"trusted":true},"outputs":[],"source":["# data = pd.read_csv('/kaggle/input/data-csv/data.csv', dtype=str)\n","# data = pd.read_csv('/Users/fernando/PyNetSim/tutorials/surrogate/data/data.csv', dtype=str)\n","data = pd.read_csv(\n","    '/Users/ffjla/PyNetSim/tutorials/surrogate/cluster_heads/data/data.csv', dtype=str)"]},{"cell_type":"markdown","metadata":{},"source":["Lets check the shape of the dataframe.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.783362Z","iopub.status.busy":"2023-11-13T22:30:22.782931Z","iopub.status.idle":"2023-11-13T22:30:22.790238Z","shell.execute_reply":"2023-11-13T22:30:22.788972Z","shell.execute_reply.started":"2023-11-13T22:30:22.783327Z"},"trusted":true},"outputs":[],"source":["print(data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Lets get only the first 10% of the data to speed up the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.792492Z","iopub.status.busy":"2023-11-13T22:30:22.792006Z","iopub.status.idle":"2023-11-13T22:30:22.863331Z","shell.execute_reply":"2023-11-13T22:30:22.861901Z","shell.execute_reply.started":"2023-11-13T22:30:22.792456Z"},"trusted":true},"outputs":[],"source":["# print data set info\n","print(data.info())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Dataframe to hold mean and std of each column\n","data_stats = pd.DataFrame(columns=['name', 'mean', 'std'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the mean and std of each column\n","name_values = data['name']\n","name_values = name_values.apply(lambda x: eval(x))\n","alpha_values = name_values.apply(lambda x: x[0])\n","beta_values = name_values.apply(lambda x: x[1])\n","gamma_values = name_values.apply(lambda x: x[2])\n","name_stats_dict = {\n","    'alpha_mean': alpha_values.mean(),\n","    'alpha_std': alpha_values.std(),\n","    'beta_mean': beta_values.mean(),\n","    'beta_std': beta_values.std(),\n","    'gamma_mean': gamma_values.mean(),\n","    'gamma_std': gamma_values.std()\n","}\n","# Lets add to the dataframe\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'alpha', 'mean': alpha_values.mean(), 'std': alpha_values.std()}, index=[0])])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'beta', 'mean': beta_values.mean(), 'std': beta_values.std()}, index=[0])])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    {'name': 'gamma', 'mean': gamma_values.mean(), 'std': gamma_values.std()}, index=[0])])\n","# reset index\n","data_stats = data_stats.reset_index(drop=True)\n","print(data_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_stats(name, data):\n","    data = data.apply(lambda x: eval(x))\n","    data_mean = data.mean()\n","    data_std = data.std()\n","    data_stats_dict = {\n","        'name': name,\n","        'mean': data_mean,\n","        'std': data_std\n","    }\n","    return data_stats_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_array_stats(name, data):\n","    data = data.apply(lambda x: eval(x))\n","    data_mean = data.apply(lambda x: np.mean(x)).mean()\n","    data_std = data.apply(lambda x: np.std(x)).mean()\n","    data_stats_dict = {\n","        'name': name,\n","        'mean': data_mean,\n","        'std': data_std\n","    }\n","    return data_stats_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get the remaining_energy column\n","remaining_energy_stats_dict = compute_stats('re',\n","                                            data['remaining_energy'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    remaining_energy_stats_dict, index=[0])]).reset_index(drop=True)\n","chs_dict = compute_array_stats('chs', data['cluster_heads'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    chs_dict, index=[0])]).reset_index(drop=True)\n","el_dict = compute_array_stats('el', data['energy_levels'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    el_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_ch_to_sink_dict = compute_array_stats(\n","    'energy_dissipated_ch_to_sink', data['energy_dissipated_ch_to_sink'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_ch_to_sink_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_non_ch_to_ch_dict = compute_array_stats(\n","    'energy_dissipated_non_ch_to_ch', data['energy_dissipated_non_ch_to_ch'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_non_ch_to_ch_dict, index=[0])]).reset_index(drop=True)\n","energy_dissipated_ch_rx_from_non_ch_dict = compute_array_stats(\n","    'energy_dissipated_ch_rx_from_non_ch', data['energy_dissipated_ch_rx_from_non_ch'])\n","data_stats = pd.concat([data_stats, pd.DataFrame(\n","    energy_dissipated_ch_rx_from_non_ch_dict, index=[0])]).reset_index(drop=True)\n","print(data_stats.to_string())"]},{"cell_type":"markdown","metadata":{},"source":["Proportion of the data that will be used for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Standardize data using F1 score\n","def standardize_inputs(x, mean, std):\n","    standardized_x = (x - mean) / std\n","    return standardized_x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.899390Z","iopub.status.busy":"2023-11-13T22:30:22.898998Z","iopub.status.idle":"2023-11-13T22:31:33.422421Z","shell.execute_reply":"2023-11-13T22:31:33.421109Z","shell.execute_reply.started":"2023-11-13T22:30:22.899356Z"},"trusted":true},"outputs":[],"source":["def split_sequence(sequence, n_steps):\n","    x_data = []\n","    y_data = []\n","    num_samples = len(sequence)\n","\n","    for i in tqdm(range(num_samples), desc=\"Processing sequence\"):\n","        end_ix = i + n_steps\n","        if end_ix > num_samples - 1:\n","            break\n","        # Get the alpha, beta, gamma\n","        name = sequence['name'][i]\n","        name = eval(name)\n","        alpha = name[0]\n","        beta = name[1]\n","        gamma = name[2]\n","\n","        # Last alpha, beta, gamma\n","        last_name = sequence['name'][end_ix]\n","        last_name = eval(last_name)\n","        last_alpha = last_name[0]\n","        last_beta = last_name[1]\n","        last_gamma = last_name[2]\n","\n","        if alpha != last_alpha or beta != last_beta or gamma != last_gamma:\n","            continue\n","\n","        alpha = standardize_inputs(alpha, data_stats.loc[data_stats['name'] == 'alpha']['mean'].values[0],\n","                                   data_stats.loc[data_stats['name'] == 'alpha']['std'].values[0])\n","        beta = standardize_inputs(beta, data_stats.loc[data_stats['name'] == 'beta']['mean'].values[0],\n","                                  data_stats.loc[data_stats['name'] == 'beta']['std'].values[0])\n","        gamma = standardize_inputs(gamma, data_stats.loc[data_stats['name'] == 'gamma']['mean'].values[0],\n","                                   data_stats.loc[data_stats['name'] == 'gamma']['std'].values[0])\n","        # print(f\"alpha: {alpha}, beta: {beta}, gamma: {gamma}\")\n","        if n_steps <= 1:\n","            remaining_energy = sequence['remaining_energy'][i]\n","            remaining_energy = eval(remaining_energy)\n","            remaining_energy = [float(remaining_energy)]\n","            pchs = sequence['potential_cluster_heads'][i]\n","            pchs = eval(pchs)\n","            pchs = [[int(x) for x in pchs]]\n","            energy_levels = sequence['energy_levels'][i]\n","            energy_levels = eval(energy_levels)\n","            energy_levels = [[float(x) for x in energy_levels]]\n","            energy_dissipated_ch_to_sink = sequence['energy_dissipated_ch_to_sink'][i]\n","            energy_dissipated_ch_to_sink = eval(energy_dissipated_ch_to_sink)\n","            energy_dissipated_ch_to_sink = [\n","                [float(x) for x in energy_dissipated_ch_to_sink]]\n","            energy_dissipated_non_ch_to_ch = sequence['energy_dissipated_non_ch_to_ch'][i]\n","            energy_dissipated_non_ch_to_ch = eval(\n","                energy_dissipated_non_ch_to_ch)\n","            energy_dissipated_non_ch_to_ch = [\n","                [float(x) for x in energy_dissipated_non_ch_to_ch]]\n","            energy_dissipated_ch_rx_from_non_ch = sequence['energy_dissipated_ch_rx_from_non_ch'][i]\n","            energy_dissipated_ch_rx_from_non_ch = eval(\n","                energy_dissipated_ch_rx_from_non_ch)\n","            energy_dissipated_ch_rx_from_non_ch = [\n","                [float(x) for x in energy_dissipated_ch_rx_from_non_ch]]\n","            num_cluster_heads = sequence['alive_nodes'][i]\n","            num_cluster_heads = eval(num_cluster_heads)\n","            # lets round up to the nearest integer\n","            num_cluster_heads = [(int(num_cluster_heads*0.05)+1)/5]\n","            cluster_heads = sequence['cluster_heads'][i]\n","            cluster_heads = eval(cluster_heads)\n","            cluster_heads = [int(x) for x in cluster_heads]\n","        else:\n","            raise f\"n_steps: {n_steps} not supported\"\n","        # print(f\"remaining_energy: {remaining_energy}\")\n","        # print(f\"pchs: {pchs}\")\n","        # print(f\"energy_levels: {energy_levels}\")\n","        # print(\n","        #     f\"energy_dissipated_ch_to_sink: {energy_dissipated_ch_to_sink}\")\n","        # print(\n","        #     f\"energy_dissipated_non_ch_to_ch: {energy_dissipated_non_ch_to_ch}\")\n","        # print(\n","        #     f\"energy_dissipated_ch_rx_from_non_ch: {energy_dissipated_ch_rx_from_non_ch}\")\n","        # print(f\"num_cluster_heads: {num_cluster_heads}\")\n","        # print(f\"cluster_heads: {cluster_heads}\")\n","        # Standardize remaining energy\n","        remaining_energy = standardize_inputs(\n","            remaining_energy, data_stats.loc[data_stats['name']\n","                                             == 're']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 're']['std'].values[0])\n","\n","        seq_y = sequence['cluster_heads_index'][end_ix-1]\n","        seq_y = eval(seq_y)\n","        seq_y = [int(x) for x in seq_y]\n","\n","        energy_levels = standardize_inputs(\n","            energy_levels, data_stats.loc[data_stats['name']\n","                                          == 'el']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 'el']['std'].values[0])\n","\n","        energy_dissipated_ch_to_sink = standardize_inputs(\n","            energy_dissipated_ch_to_sink, data_stats.loc[data_stats['name']\n","                                                         == 'energy_dissipated_ch_to_sink']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_ch_to_sink']['std'].values[0])\n","\n","        energy_dissipated_non_ch_to_ch = standardize_inputs(\n","            energy_dissipated_non_ch_to_ch, data_stats.loc[data_stats['name']\n","                                                           == 'energy_dissipated_non_ch_to_ch']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_non_ch_to_ch']['std'].values[0])\n","\n","        energy_dissipated_ch_rx_from_non_ch = standardize_inputs(\n","            energy_dissipated_ch_rx_from_non_ch, data_stats.loc[data_stats['name']\n","                                                                == 'energy_dissipated_ch_rx_from_non_ch']['mean'].values[0],\n","            data_stats.loc[data_stats['name']\n","                           == 'energy_dissipated_ch_rx_from_non_ch']['std'].values[0])\n","\n","        cluster_heads = standardize_inputs(\n","            cluster_heads, data_stats.loc[data_stats['name']\n","                                          == 'chs']['mean'].values[0],\n","            data_stats.loc[data_stats['name'] == 'chs']['std'].values[0])\n","        # print(f\"cluster_heads: {cluster_heads}\")\n","        seq_x = []\n","        if n_steps <= 1:\n","            seq_x.extend([alpha, beta, gamma])\n","            seq_x.extend(remaining_energy)\n","            seq_x.extend(pchs[0])\n","            seq_x.extend(energy_levels[0])\n","            seq_x.extend(energy_dissipated_ch_to_sink[0])\n","            # seq_x.extend(energy_dissipated_non_ch_to_ch[0])\n","            seq_x.extend(energy_dissipated_ch_rx_from_non_ch[0])\n","            seq_x.extend(num_cluster_heads)\n","        else:\n","            raise f\"n_steps: {n_steps} not supported\"\n","        # print(f\"seq_x: {seq_x}\")\n","        # print(f\"len(seq_x): {len(seq_x)}\")\n","        # print(f\"seq_y: {seq_y}\")\n","        # return\n","\n","        x_data.append(seq_x)\n","        y_data.append(seq_y)\n","\n","    return np.array(x_data), np.array(y_data)\n","\n","\n","n_steps = 1\n","x_data, y_data = split_sequence(data, n_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print the array where the first three rows are 0.8485311004611152, 0.6836234887293634, -0.4393387688162076\n","needed_data = np.where(x_data[:, 0] == 0.8485311004611152)\n","# print the entire row\n","print(list(x_data[needed_data]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np_x = np.array(x_data)\n","print(f\"np_x.shape: {np_x.shape}\")\n","np_y = np.array(y_data)\n","print(np_y.shape)\n","# np_y = np.argmax(np_y, axis=2)\n","# print(np_y.shape)\n","# print(np_y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into train and test\n","x_train, x_test, y_train, y_test = train_test_split(\n","    np_x, np_y, test_size=0.1, random_state=42, shuffle=True)"]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset class."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.432915Z","iopub.status.busy":"2023-11-13T22:31:33.432564Z","iopub.status.idle":"2023-11-13T22:31:33.446920Z","shell.execute_reply":"2023-11-13T22:31:33.445701Z","shell.execute_reply.started":"2023-11-13T22:31:33.432885Z"},"trusted":true},"outputs":[],"source":["class ClusterHeadDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.X = torch.from_numpy(x.astype(np.float32))\n","        self.y = torch.from_numpy(y.astype(np.float32))\n","        self.len = x.shape[0]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","    # Support batching\n","    def collate_fn(self, batch):\n","        X = torch.stack([x[0] for x in batch])\n","        y = torch.stack([x[1] for x in batch])\n","        return X, y"]},{"cell_type":"markdown","metadata":{},"source":["Create the network architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.450481Z","iopub.status.busy":"2023-11-13T22:31:33.449246Z","iopub.status.idle":"2023-11-13T22:31:33.465721Z","shell.execute_reply":"2023-11-13T22:31:33.464591Z","shell.execute_reply.started":"2023-11-13T22:31:33.450418Z"},"trusted":true},"outputs":[],"source":["class ForecastCCH(nn.Module):\n","    global non_cyclical_features_size, cyclical_features_size\n","\n","    def __init__(self, input_size=10, h1=100, h2=100, output_size=101):\n","        super(ForecastCCH, self).__init__()\n","        self.batch_norm1 = nn.BatchNorm1d(input_size)\n","        self.fc1 = nn.Linear(input_size, h1)\n","        self.batch_norm2 = nn.BatchNorm1d(h1)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.3)\n","\n","        self.fc2 = nn.Linear(h1, h2)\n","        self.batch_norm3 = nn.BatchNorm1d(h2)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.4)\n","\n","        self.fc3 = nn.Linear(h2, output_size)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # print(f\"x shape: {x.shape}\")\n","        out = self.batch_norm1(x)\n","        out = self.fc1(x)\n","        out = self.batch_norm2(out)\n","        out = self.relu1(out)\n","        out = self.dropout1(out)\n","        # print(f\"out shape1: {out.shape}\")\n","\n","        out = self.fc2(out)\n","        out = self.batch_norm3(out)\n","        out = self.relu2(out)\n","        out = self.dropout2(out)\n","        # print(f\"out shape2: {out.shape}\")\n","\n","        out = self.fc3(out)\n","        out = self.sigmoid(out)\n","        # print(f\"out shape3: {out.shape}\")\n","\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.468837Z","iopub.status.busy":"2023-11-13T22:31:33.467832Z","iopub.status.idle":"2023-11-13T22:31:33.508796Z","shell.execute_reply":"2023-11-13T22:31:33.507729Z","shell.execute_reply.started":"2023-11-13T22:31:33.468774Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = ForecastCCH(\n","    input_size=401, h1=2000, h2=2000, output_size=101).to(device)\n","# # If there is a model saved, load it\n","if os.path.isfile('ch_model.pt'):\n","    model.load_state_dict(torch.load('ch_model.pt'))\n","    print(\"Model loaded\")\n","else:\n","    print(\"No model found\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-6, weight_decay=1e-5)\n","criterion = nn.BCELoss()\n","rl_scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset objects."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.511959Z","iopub.status.busy":"2023-11-13T22:31:33.511069Z","iopub.status.idle":"2023-11-13T22:31:33.588626Z","shell.execute_reply":"2023-11-13T22:31:33.587611Z","shell.execute_reply.started":"2023-11-13T22:31:33.511919Z"},"trusted":true},"outputs":[],"source":["train = ClusterHeadDataset(x_train, y_train)\n","valid = ClusterHeadDataset(x_test, y_test)\n","train_loader = DataLoader(train, batch_size=16, shuffle=True)\n","valid_loader = DataLoader(valid, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_accuracy(y_true, y_prob, print_info=False):\n","    assert y_true.ndim == 1 and y_true.size() == y_prob.size()\n","    y_prob = y_prob > 0.5\n","    if print_info:\n","        print(f\"y_true: {np.where(y_true == 1)}\")\n","        print(f\"y_prob: {np.where(y_prob == 1)}\")\n","        # delay 1 second\n","        time.sleep(1)\n","    return (y_true == y_prob).sum().item() / y_true.size(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_accuracy_top_five(y_true, y_prob, print_info=False):\n","    assert y_true.ndim == 1 and y_true.size() == y_prob.size()\n","    y_prob = y_prob.cpu().numpy()\n","    y_prob = y_prob.argsort()[::-1][:5]\n","    # sort\n","    y_prob.sort()\n","    if print_info:\n","        y_true = y_true.cpu().numpy()\n","        y_true = np.where(y_true == 1)[0]\n","        # Check how many common elements are in both arrays\n","        correct = np.intersect1d(y_true, y_prob)\n","        # sum the number of correct elements\n","        correct = correct.size\n","        total = y_true.size\n","        if correct < 5:\n","            print(f\"y_true: {y_true}\")\n","            print(f\"y_prob: {y_prob}\")\n","            # delay 1 second\n","            # time.sleep(1)\n","        # delay 1 second\n","        # time.sleep(1)\n","    return correct / total"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test_predicted(print_info=False):\n","    model.eval()\n","    avg_accuracy = []\n","    losses = []\n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(valid_loader):\n","            # print(f\"inputs: {inputs}, shape: {inputs.shape}\")\n","            # print(f\"labels: {labels}, shape: {labels.shape}\")\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            # print(f\"labels: {labels}, shape: {labels.shape}\")\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            # print(f\"preds: {preds}, shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            losses.append(loss.item())\n","            temp_accuracy = []\n","            # Loop over both the predictions and the labels\n","            for pred, label in zip(preds, labels):\n","                # print(f\"pred: {pred}, shape: {pred.shape}\")\n","                # print(f\"label: {label}, shape: {label.shape}\")\n","                accuracy = get_accuracy(label, pred, print_info)\n","                temp_accuracy.append(accuracy*100)\n","            avg_accuracy.append(np.mean(temp_accuracy))\n","    print(\n","        f\"Average loss: {np.mean(losses)}\")\n","    print(\n","        f\"Average accuracy: {np.mean(avg_accuracy)}%\")\n","    print(\n","        f\"Min Accuracy: {np.min(avg_accuracy)}%\")\n","    # number of samples with the lowest accuracy\n","    min_accuracy = np.min(avg_accuracy)\n","    min_accuracy_count = np.count_nonzero(avg_accuracy == min_accuracy)\n","    print(\n","        f\"Min Accuracy count: {min_accuracy_count}\")\n","    print(\n","        f\"Max Accuracy: {np.max(avg_accuracy)}%\")\n","    # number of samples with the highest accuracy\n","    max_accuracy = np.max(avg_accuracy)\n","    max_accuracy_count = np.count_nonzero(avg_accuracy == max_accuracy)\n","    print(\n","        f\"Max Accuracy count: {max_accuracy_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test(epoch, epochs, best_loss):\n","    running_loss = .0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(valid_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            # print(f\"test inputs shape: {inputs.shape}\")\n","            # print(f\"test labels shape: {labels.shape}\")\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            preds = preds.squeeze()\n","            # print(f\"test preds shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            running_loss += loss\n","\n","        valid_loss = running_loss/len(valid_loader)\n","        # print(f'valid_loss {valid_loss}')\n","\n","        if valid_loss < best_loss:\n","            print(\n","                f\"Epoch [{epoch}/{epochs}] Validation Loss Improved: {best_loss:.4f} -> {valid_loss:.4f}\")\n","            best_loss = valid_loss\n","            torch.save(model.state_dict(), 'ch_model.pt')\n","\n","    return best_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.592841Z","iopub.status.busy":"2023-11-13T22:31:33.592324Z","iopub.status.idle":"2023-11-13T22:31:33.605727Z","shell.execute_reply":"2023-11-13T22:31:33.604423Z","shell.execute_reply.started":"2023-11-13T22:31:33.592795Z"},"trusted":true},"outputs":[],"source":["\n","\n","def train():\n","    train_losses = []\n","    best_loss = float('inf')\n","    epochs = 1000\n","    for epoch in range(epochs):\n","        print(\n","            f'epochs {epoch}/{epochs}, LR: {optimizer.param_groups[0][\"lr\"]}')\n","        model.train()\n","        training_loss = .0\n","        # Wrap the data loader with tqdm to add a progress bar\n","        for idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n","            # print(f\"inputs shape: {inputs.shape}\")\n","            # print(f\"labels shape: {labels.shape}\")\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            # preds = preds.squeeze()\n","            # print(f\"predes shape: {preds.shape}\")\n","            loss = criterion(preds, labels)\n","            loss.backward()\n","            optimizer.step()\n","            training_loss += loss\n","\n","        train_loss = training_loss / len(train_loader)\n","        train_losses.append(train_loss.detach().numpy())\n","        print(f'train_loss {train_loss}')\n","        rl_scheduler.step()\n","\n","        if epoch % 5 == 0:\n","            best_loss = test(epoch, epochs, best_loss)\n","\n","        if epoch % 5 == 0:\n","            test_predicted()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.627847Z","iopub.status.busy":"2023-11-13T22:31:33.627155Z"},"trusted":true},"outputs":[],"source":["\n","train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["valid_loader = DataLoader(valid, batch_size=1, shuffle=True)\n","test_predicted(False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","def custom_annot(value):\n","    return f'{value}' if value != 0 else ''\n","# Another approach to do the confusion matrix\n","y_true = []\n","y_pred = []\n","with torch.no_grad():\n","    for idx, (inputs, labels) in enumerate(valid_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        labels = labels.cpu().numpy()\n","        labels = np.where(labels == 1)[1]\n","        # lables to int\n","        labels = labels.astype(int)\n","        preds = model(inputs.float())\n","        preds = preds.squeeze()\n","        preds = preds > 0.5\n","        preds = preds.cpu().numpy()\n","        predicted = np.where(preds == 1)[0]\n","        if len(predicted) > len(labels):\n","            indices = preds.argsort()[::-1]\n","            predicted_index = indices[:len(labels)]\n","            predicted_index.sort()\n","            labels_not_in_pred = np.setdiff1d(labels, predicted_index)\n","            pred_not_in_labels = np.setdiff1d(predicted_index, labels)\n","            labels_not_in_pred_pos = np.where(\n","                np.isin(labels, labels_not_in_pred))[0]\n","            pred_not_in_labels_pos = np.where(\n","                np.isin(predicted_index, pred_not_in_labels))[0]\n","            labels[labels_not_in_pred_pos] = predicted_index[pred_not_in_labels_pos]\n","            predicted_index = labels\n","            predicted = predicted_index\n","        elif len(predicted) < len(labels):\n","            predicted = np.append(\n","                predicted, np.zeros(len(labels)-len(predicted)))\n","            predicted_index = predicted\n","            predicted_index.sort()\n","            labels_not_in_pred = np.setdiff1d(labels, predicted_index)\n","            pred_not_in_labels = np.setdiff1d(predicted_index, labels)\n","            labels_not_in_pred_pos = np.where(\n","                np.isin(labels, labels_not_in_pred))[0]\n","            pred_not_in_labels_pos = np.where(\n","                np.isin(predicted_index, pred_not_in_labels))[0]\n","            labels[labels_not_in_pred_pos] = predicted_index[pred_not_in_labels_pos]\n","            predicted_index = labels\n","            predicted = predicted_index\n","        if len(labels) < 5:\n","            labels = np.append(labels, np.zeros(5-len(labels)))\n","            predicted = np.append(predicted, np.zeros(5-len(predicted)))\n","            labels.sort()\n","            predicted.sort()\n","        # predicted as int\n","        predicted = predicted.astype(int)\n","        y_true.append(labels)\n","        y_pred.append(predicted)\n","y_true = np.array(y_true)\n","y_pred = np.array(y_pred)\n","print(y_true.shape)\n","print(y_pred.shape)\n","# lest print the first 10 elements\n","print(y_true[:10])\n","print(y_pred[:10])\n","#  Find unique classes in the dataset\n","unique_classes = np.unique(np.concatenate((y_true, y_pred)))\n","# convert to int\n","unique_classes = unique_classes.astype(int)\n","print(unique_classes)\n","# Initialize an overall confusion matrix\n","overall_cm = np.zeros(\n","    (len(unique_classes), len(unique_classes)), dtype=np.int64)\n","print(overall_cm)\n","# Loop through each pair of true and predicted values\n","for true_label, pred_label in zip(y_true, y_pred):\n","    cm = confusion_matrix(true_label, pred_label, labels=unique_classes)\n","    overall_cm += cm\n","# Calculate row and column percentages\n","row_percentages = overall_cm / overall_cm.sum(axis=1, keepdims=True)\n","col_percentages = overall_cm / overall_cm.sum(axis=0, keepdims=True)\n","\n","# Plot the confusion matrix using seaborn\n","fig, ax = plt.subplots(figsize=(40, 40))\n","\n","# Use a custom annotation function\n","annot_values = np.vectorize(custom_annot)(overall_cm)\n","# Mask non-diagonal elements\n","mask = np.eye(len(unique_classes), dtype=bool)\n","\n","\n","# Plot the heatmap\n","sns.heatmap(overall_cm, annot=False, fmt='', cmap='Blues',\n","            xticklabels=unique_classes, yticklabels=unique_classes, ax=ax,\n","            annot_kws={\"size\": 27}, mask=~mask,\n","            cbar=False)\n","\n","# Add row percentages on the right side\n","for i, row in enumerate(row_percentages):\n","    ax.text(len(unique_classes) + 1.5, i + 0.5,\n","            f'{row.max() * 100:.1f}%', ha='center', va='center', fontsize=27)\n","\n","# Add vertical column percentages at the bottom\n","for i, col in enumerate(col_percentages.T):\n","    ax.text(i + 0.5, -2, f'{col.max() * 100:.1f}%',\n","            ha='center', va='center', rotation=90, fontsize=27)\n","\n","# set ticks of the y axis to be horizontal\n","ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n","\n","# Increase the gap between the ticks and the annotations\n","\n","# Adjust the layout to increase the gap\n","# plt.subplots_adjust(right=1, bottom=0.05)\n","\n","# plt.title('Confusion Matrix with Row and Column Percentages')\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","# set the font size of the x and y labels\n","ax.xaxis.label.set_size(40)\n","ax.yaxis.label.set_size(40)\n","# Change the orientation of the x ticks\n","plt.xticks(rotation=90)\n","# Set the font size of the tick labels\n","ax.tick_params(labelsize=27)\n","# Set the font size of the heatmap annotations\n","# tight\n","plt.tight_layout()\n","# save the confusion matrix\n","plt.savefig('confusion_matrix.pdf')\n","plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load a sample given alpha, beta, gamma values\n","alpha_values = 54.82876630831832\n","beta_values = 14.53707859358856\n","gamma_values = 35.31010127750784\n","# filter out the data with the given alpha, beta, gamma values\n","filtered_data = data[(data['name'].str.contains(\n","    str(alpha_values))) & (data['name'].str.contains(str(beta_values))) & (data['name'].str.contains(str(gamma_values)))]\n","filtered_data = filtered_data.copy().reset_index(drop=True)\n","x_data, y_data = split_sequence(filtered_data, 1)\n","print(f\"x_data shape: {x_data.shape}\")\n","print(f\"y_data shape: {y_data.shape}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_to_predict = ClusterHeadDataset(x_data, y_data)\n","valid_loader = DataLoader(data_to_predict, batch_size=1, shuffle=False)\n","test_predicted(True)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3998218,"sourceId":6960150,"sourceType":"datasetVersion"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
