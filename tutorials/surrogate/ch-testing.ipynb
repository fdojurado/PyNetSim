{"cells":[{"cell_type":"code","execution_count":402,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:19.337147Z","iopub.status.busy":"2023-11-13T22:30:19.336301Z","iopub.status.idle":"2023-11-13T22:30:19.351617Z","shell.execute_reply":"2023-11-13T22:30:19.350253Z","shell.execute_reply.started":"2023-11-13T22:30:19.337101Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import lr_scheduler\n","from tqdm import tqdm\n","import torch.nn as nn\n","import gc\n","import torch\n","from numpy import array\n","import numpy as np  # linear algebra\n","import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."]},{"cell_type":"markdown","metadata":{},"source":["Lets load the CSV file into a Pandas dataframe."]},{"cell_type":"code","execution_count":403,"metadata":{"execution":{"iopub.execute_input":"2023-11-14T09:39:09.239361Z","iopub.status.busy":"2023-11-14T09:39:09.238971Z","iopub.status.idle":"2023-11-14T09:39:09.657707Z","shell.execute_reply":"2023-11-14T09:39:09.656089Z","shell.execute_reply.started":"2023-11-14T09:39:09.239330Z"},"trusted":true},"outputs":[],"source":["# data = pd.read_csv('/kaggle/input/data-csv/data.csv', dtype=str)\n","# data = pd.read_csv('/Users/fernando/PyNetSim/tutorials/surrogate/data/data.csv', dtype=str)\n","data = pd.read_csv(\n","    '/Users/ffjla/PyNetSim/tutorials/surrogate/data/data.csv', dtype=str)"]},{"cell_type":"markdown","metadata":{},"source":["Lets check the shape of the dataframe.\n"]},{"cell_type":"code","execution_count":404,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.783362Z","iopub.status.busy":"2023-11-13T22:30:22.782931Z","iopub.status.idle":"2023-11-13T22:30:22.790238Z","shell.execute_reply":"2023-11-13T22:30:22.788972Z","shell.execute_reply.started":"2023-11-13T22:30:22.783327Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(76460, 16)\n"]}],"source":["print(data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Lets get only the first 10% of the data to speed up the training process."]},{"cell_type":"code","execution_count":405,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.792492Z","iopub.status.busy":"2023-11-13T22:30:22.792006Z","iopub.status.idle":"2023-11-13T22:30:22.863331Z","shell.execute_reply":"2023-11-13T22:30:22.861901Z","shell.execute_reply.started":"2023-11-13T22:30:22.792456Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation set shape:  (7646, 16)\n"]}],"source":["num_samples = len(data)\n","# Validation is the last 10% of samples\n","validation_set = data[int(num_samples*0.9):]\n","# reset index in both dataframes\n","validation_set.reset_index(drop=True, inplace=True)\n","# print shapes\n","print('Validation set shape: ', validation_set.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Proportion of the data that will be used for training and testing."]},{"cell_type":"code","execution_count":406,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.868083Z","iopub.status.busy":"2023-11-13T22:30:22.867591Z","iopub.status.idle":"2023-11-13T22:30:22.875409Z","shell.execute_reply":"2023-11-13T22:30:22.874239Z","shell.execute_reply.started":"2023-11-13T22:30:22.868046Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Proportion of validation set: 10.0%\n"]}],"source":["print(f\"Proportion of validation set: {validation_set.shape[0]/len(data)*100}%\")"]},{"cell_type":"code","execution_count":407,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.877457Z","iopub.status.busy":"2023-11-13T22:30:22.877018Z","iopub.status.idle":"2023-11-13T22:30:22.896951Z","shell.execute_reply":"2023-11-13T22:30:22.896044Z","shell.execute_reply.started":"2023-11-13T22:30:22.877390Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["               alpha               beta               gamma  \\\n","0  6.396204291486541  3.317884312892152  2.3993866139171405   \n","1  6.396204291486541  3.317884312892152  2.3993866139171405   \n","2  6.396204291486541  3.317884312892152  2.3993866139171405   \n","3  6.396204291486541  3.317884312892152  2.3993866139171405   \n","4  6.396204291486541  3.317884312892152  2.3993866139171405   \n","\n","     remaining_energy alive_nodes         cluster_heads  \\\n","0   2.845430725207556          89  [33, 49, 56, 70, 88]   \n","1   2.801312292226756          89     [2, 4, 8, 25, 72]   \n","2   2.757201859245956          89    [4, 8, 72, 78, 88]   \n","3   2.713493629049157          89   [4, 22, 58, 59, 72]   \n","4  2.6706714707487573          89  [17, 25, 30, 78, 91]   \n","\n","                                       energy_levels  \\\n","0  [0.03218247005397702, 0.02710971871302542, 0.0...   \n","1  [0.02885879005397704, 0.026880038713025422, 0....   \n","2  [0.02863811005397704, 0.026650358713025422, 0....   \n","3  [0.02841523005397704, 0.026420678713025423, 0....   \n","4  [0.02819263005397704, 0.026159678713025422, 0....   \n","\n","                                 dst_to_cluster_head  \\\n","0  [4.123105625617661, 23.853720883753127, 12.649...   \n","1  [0.0, 15.556349186104045, 0.0, 12.529964086141...   \n","2  [4.123105625617661, 15.556349186104045, 0.0, 1...   \n","3  [8.48528137423857, 15.556349186104045, 0.0, 12...   \n","4  [8.06225774829855, 32.01562118716424, 28.30194...   \n","\n","                                          membership                  eelect  \\\n","0  [88, 33, 70, 49, 88, 49, 33, 0, 70, 49, 70, 49...  5.0000000000000004e-08   \n","1  [2, 72, 4, 72, 2, 72, 8, 0, 4, 4, 4, 72, 8, 2,...  5.0000000000000004e-08   \n","2  [88, 72, 4, 72, 88, 72, 8, 0, 4, 4, 4, 72, 8, ...  5.0000000000000004e-08   \n","3  [22, 72, 4, 72, 22, 72, 58, 0, 4, 4, 4, 72, 58...  5.0000000000000004e-08   \n","4  [25, 17, 91, 91, 25, 91, 17, 0, 91, 91, 78, 91...  5.0000000000000004e-08   \n","\n","  pkt_size                    eamp    efs    eda                 d0  \\\n","0     4000  1.2999999999999998e-15  1e-11  5e-09  87.70580193070293   \n","1     4000  1.2999999999999998e-15  1e-11  5e-09  87.70580193070293   \n","2     4000  1.2999999999999998e-15  1e-11  5e-09  87.70580193070293   \n","3     4000  1.2999999999999998e-15  1e-11  5e-09  87.70580193070293   \n","4     4000  1.2999999999999998e-15  1e-11  5e-09  87.70580193070293   \n","\n","                               avg_min_max_distances  \n","0  [101.77318365941255, 9.865116374119573, 190.16...  \n","1  [101.77318365941255, 9.865116374119573, 190.16...  \n","2  [101.77318365941255, 9.865116374119573, 190.16...  \n","3  [101.77318365941255, 9.865116374119573, 190.16...  \n","4  [101.77318365941255, 9.865116374119573, 190.16...  ,(7646, 16)\n"]}],"source":["print(f\"{validation_set.head()},{validation_set.shape}\")"]},{"cell_type":"code","execution_count":408,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:30:22.899390Z","iopub.status.busy":"2023-11-13T22:30:22.898998Z","iopub.status.idle":"2023-11-13T22:31:33.422421Z","shell.execute_reply":"2023-11-13T22:31:33.421109Z","shell.execute_reply.started":"2023-11-13T22:30:22.899356Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing sequence: 100%|█████████▉| 7641/7646 [00:15<00:00, 497.81it/s]\n"]}],"source":["def split_sequence(sequence, n_steps):\n","    x_data = []\n","    y_data = []\n","    num_samples = len(sequence)\n","\n","    # Get the eelect, pkt_size, eamp, efs, eda, d0\n","    eelect = sequence['eelect'][0]\n","    eelect = float(eelect)\n","    pkt_size = sequence['pkt_size'][0]\n","    pkt_size = float(pkt_size)/4000\n","    eamp = sequence['eamp'][0]\n","    eamp = float(eamp)\n","    efs = sequence['efs'][0]\n","    efs = float(efs)\n","    eda = sequence['eda'][0]\n","    eda = float(eda)\n","    d0 = sequence['d0'][0]\n","    d0 = float(d0)/100\n","\n","    avg_min_max_distances = sequence['avg_min_max_distances'].values[0]\n","    avg_min_max_distances = eval(avg_min_max_distances)\n","    avg_min_max_distances = [float(x)/300 for x in avg_min_max_distances]\n","\n","    for i in tqdm(range(num_samples), desc=\"Processing sequence\"):\n","        end_ix = i + n_steps\n","        if end_ix > num_samples - 1:\n","            break\n","        alpha_val, beta_val, gamma_val = sequence['alpha'][i:end_ix].values, sequence['beta'][\n","            i:end_ix].values, sequence['gamma'][i:end_ix].values\n","        # convert to float\n","        alpha_val = [float(x)/10 for x in alpha_val]\n","        beta_val = [float(x)/10 for x in beta_val]\n","        gamma_val = [float(x)/10 for x in gamma_val]\n","        assert all(\n","            x <= 1 and x >= -1 for x in alpha_val), f\"Incorrect values of alpha: {alpha_val}\"\n","        assert all(\n","            x <= 1 and x >= -1 for x in beta_val), f\"Incorrect values of beta: {beta_val}\"\n","        assert all(\n","            x <= 1 and x >= -1 for x in gamma_val), f\"Incorrect values of gamma: {gamma_val}\"\n","        # Normalize remaining energy dividing by 10\n","        remaining_energy = sequence['remaining_energy'][i:end_ix]\n","        remaining_energy = [float(x)/10 for x in remaining_energy]\n","        assert all(\n","            x <= 1 and x >= -1 for x in remaining_energy), f\"Incorrect values of remaining energy: {remaining_energy}\"\n","        # seq_x.extend(remaining_energy)\n","        # Normalize alive nodes dividing by 100\n","        alive_nodes = sequence['alive_nodes'][i:end_ix].values\n","        alive_nodes = [float(x)/100 for x in alive_nodes]\n","        assert all(\n","            x <= 1 and x >= -1 for x in alive_nodes), f\"Incorrect values of alive nodes: {alive_nodes}\"\n","        # seq_x.extend(alive_nodes)\n","        # Normalize energy levels dividing by 5\n","        energy_levels = sequence['energy_levels'][i:end_ix].values\n","        energy_levels = [eval(x) for x in energy_levels]\n","        # Convert to float every element in the array of arrays\n","        energy_levels = [[float(x)/5 for x in sublist]\n","                         for sublist in energy_levels]\n","        # energy levels is a list of lists, so we need to assert that all values are between -1 and 1\n","        # We iterate over the list of lists and assert that all values are between -1 and 1\n","        assert all(\n","            -1 <= x <= 1 for sublist in energy_levels for x in sublist), f\"Incorrect values of energy levels: {energy_levels}\"\n","        # seq_x.extend(energy_levels)\n","        # Normalize distance to cluster head dividing by 100\n","        dst_to_cluster_head = sequence['dst_to_cluster_head'][i:end_ix].values\n","        dst_to_cluster_head = [eval(x) for x in dst_to_cluster_head]\n","        dst_to_cluster_head = [[float(x)/200 for x in sublist]\n","                               for sublist in dst_to_cluster_head]\n","        assert all(-1 <= x <=\n","                   1 for sublist in dst_to_cluster_head for x in sublist), f\"Incorrect values of distance to cluster head: {dst_to_cluster_head}\"\n","\n","        # seq_x.extend(dst_to_cluster_head)\n","        # Normalize membership dividing by 100\n","        membership = sequence['membership'][i:end_ix].values\n","        membership = [eval(x) for x in membership]\n","        membership = [[float(x)/100 for x in sublist]\n","                      for sublist in membership]\n","        assert all(-1 <= x <=\n","                   1 for sublist in membership for x in sublist), f\"Incorrect values of membership: {membership}\"\n","        # seq_x.extend(membership)\n","        # Normalize cluster heads dividing by 100\n","        chs, seq_y = sequence['cluster_heads'][i:\n","                                               end_ix], sequence['cluster_heads'][end_ix]\n","        chs = [eval(x) for x in chs]\n","        chs = [[float(x)/100 for x in sublist] for sublist in chs]\n","        assert all(-1 <= x <=\n","                   1 for sublist in chs for x in sublist), f\"Incorrect values of cluster heads: {chs}\"\n","\n","        seq_y = eval(seq_y)\n","\n","        next_alpha_val, next_beta_val, next_gamma_val = sequence['alpha'][end_ix], sequence['beta'][\n","            end_ix], sequence['gamma'][end_ix]\n","        # convert to float\n","        next_alpha_val = float(next_alpha_val)/10\n","        next_beta_val = float(next_beta_val)/10\n","        next_gamma_val = float(next_gamma_val)/10\n","\n","        assert all(\n","            x <= 1 and x >= 0 for x in avg_min_max_distances), f\"Incorrect values of avg_min_max_distances: {avg_min_max_distances}\"\n","\n","        if (next_alpha_val != alpha_val[0]) or (next_beta_val != beta_val[0]) or (next_gamma_val != gamma_val[0]):\n","            continue\n","\n","        # Lets put the data into the seq_x like this weights[0], remaining energy[0],...,weights[1], remaining energy[1]\n","        seq_x_tmp = []\n","        for i in range(n_steps):\n","            a = alpha_val[i]\n","            b = beta_val[i]\n","            g = gamma_val[i]\n","            re = remaining_energy[i]\n","            an = alive_nodes[i]\n","            ch = chs[i]\n","            el = energy_levels[i]\n","            dst = dst_to_cluster_head[i]\n","            mem = membership[i]\n","            # Put the alpha, beta, gamma, remaining energy and alive nodes at the end of the list\n","            exp = []\n","            if i == 0:\n","                exp.extend([a, b, g, re, an])\n","                exp.extend(ch)\n","                exp.extend(el)\n","                exp.extend(dst)\n","                exp.extend(mem)\n","                exp.extend([eelect, pkt_size, eamp, efs, eda, d0])\n","                exp.extend(avg_min_max_distances)\n","            else:\n","                exp.extend([re, an])\n","                exp.extend(ch)\n","                exp.extend(el)\n","                exp.extend(dst)\n","                exp.extend(mem)\n","            # Append to the list\n","            seq_x_tmp.append(exp)\n","            # seq_x.extend([a, b, g, re, an, el, dst, mem, ch])\n","\n","        # Convert seq_x into a single list\n","        seq_x = [item for sublist in seq_x_tmp for item in sublist]\n","\n","        x_data.append(seq_x)\n","        y_data.append(seq_y)\n","\n","    return np.array(x_data), np.array(y_data)\n","\n","\n","n_steps = 5\n","x_val_split, y_val_split = split_sequence(validation_set, n_steps)"]},{"cell_type":"code","execution_count":409,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n"," 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0.]\n","[12 22 26 71 82]\n"]}],"source":["y_val = np.zeros((y_val_split.shape[0], y_val_split.max()+1))\n","# Set to 1 the index of the label\n","for i, label in enumerate(y_val_split):\n","    y_val[i][label] = 1\n","x_val = x_val_split\n","print(y_val[0])\n","# print the indeces where y_val[0] is 1\n","print(np.where(y_val[0] == 1)[0])"]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset class."]},{"cell_type":"code","execution_count":410,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.432915Z","iopub.status.busy":"2023-11-13T22:31:33.432564Z","iopub.status.idle":"2023-11-13T22:31:33.446920Z","shell.execute_reply":"2023-11-13T22:31:33.445701Z","shell.execute_reply.started":"2023-11-13T22:31:33.432885Z"},"trusted":true},"outputs":[],"source":["class ClusterHeadDataset(Dataset):\n","    def __init__(self, x, y):\n","        self.X = torch.from_numpy(x.astype(np.float32))\n","        self.y = torch.from_numpy(y.astype(np.float32))\n","        self.len = x.shape[0]\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","    # Support batching\n","    def collate_fn(self, batch):\n","        X = torch.stack([x[0] for x in batch])\n","        y = torch.stack([x[1] for x in batch])\n","        return X, y"]},{"cell_type":"markdown","metadata":{},"source":["Create the network architecture."]},{"cell_type":"code","execution_count":411,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.450481Z","iopub.status.busy":"2023-11-13T22:31:33.449246Z","iopub.status.idle":"2023-11-13T22:31:33.465721Z","shell.execute_reply":"2023-11-13T22:31:33.464591Z","shell.execute_reply.started":"2023-11-13T22:31:33.450418Z"},"trusted":true},"outputs":[],"source":["class ForecastCCH(nn.Module):\n","    def __init__(self):\n","        super(ForecastCCH, self).__init__()\n","        self.batch_norm = nn.BatchNorm1d(1826)\n","        self.fc1 = nn.Linear(1826, 3000)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(0.2)\n","        # self.batch_norm2 = nn.BatchNorm1d(2000)\n","\n","        self.fc2 = nn.Linear(3000, 4000)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(0.2)\n","        # self.batch_norm3 = nn.BatchNorm1d(4000)\n","\n","        self.fc3 = nn.Linear(4000, 2000)\n","        self.relu3 = nn.ReLU()\n","        self.dropout3 = nn.Dropout(0.2)\n","        # self.batch_norm4 = nn.BatchNorm1d(2000)\n","\n","        self.fc4 = nn.Linear(2000, 101)\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.batch_norm(x)\n","        x = self.fc1(x)\n","        # x = self.batch_norm2(x)\n","        x = self.relu1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.fc2(x)\n","        # x = self.batch_norm3(x)\n","        x = self.relu2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.fc3(x)\n","        # x = self.batch_norm4(x)\n","        x = self.relu3(x)\n","        x = self.dropout3(x)\n","\n","        x = self.fc4(x)\n","        x = self.sigmoid(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":417,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.468837Z","iopub.status.busy":"2023-11-13T22:31:33.467832Z","iopub.status.idle":"2023-11-13T22:31:33.508796Z","shell.execute_reply":"2023-11-13T22:31:33.507729Z","shell.execute_reply.started":"2023-11-13T22:31:33.468774Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model loaded\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = ForecastCCH().to(device)\n","# If there is a model saved, load it\n","if os.path.isfile('model.pt'):\n","    model.load_state_dict(torch.load('model.pt'))\n","    print(\"Model loaded\")\n","else:\n","    print(\"No model found\")"]},{"cell_type":"code","execution_count":418,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.BCELoss()\n"]},{"cell_type":"markdown","metadata":{},"source":["Create the dataset objects."]},{"cell_type":"code","execution_count":419,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.511959Z","iopub.status.busy":"2023-11-13T22:31:33.511069Z","iopub.status.idle":"2023-11-13T22:31:33.588626Z","shell.execute_reply":"2023-11-13T22:31:33.587611Z","shell.execute_reply.started":"2023-11-13T22:31:33.511919Z"},"trusted":true},"outputs":[],"source":["valid = ClusterHeadDataset(x_val, y_val)\n","valid_loader = DataLoader(valid, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":420,"metadata":{},"outputs":[],"source":["def test_predicted():\n","    model.eval()\n","    avg_accuracy = []\n","    losses = []\n","    threshold = 0.5\n","    with torch.no_grad():\n","        for idx, (inputs, labels) in enumerate(valid_loader):\n","            # print(f\"inputs: {inputs}, shape: {inputs.shape}\")\n","            # print(f\"labels: {labels}, shape: {labels.shape}\")\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            preds = model(inputs.float())\n","            loss = criterion(preds, labels)\n","            losses.append(loss.item())\n","            correct = 0\n","            total = 0\n","            # print(f\"y shape: {labels.shape}\")\n","            # print(f\"preds shape: {preds.shape}\")\n","            # Loop over both the predictions and the labels\n","            for pred, label in zip(preds, labels):\n","                # print(f\"pred: {pred}\")\n","                # print(f\"label: {label}\")\n","                # Get the indices from labels where the value is 1\n","                y = np.where(label == 1)[0]\n","                # sort the indices\n","                y.sort()\n","                # print(f\"y: {y}\")\n","                # Get the topk indices from the predictions\n","                y_hat = torch.topk(pred, len(y))\n","                # Convert y_hat to numpy\n","                y_hat = y_hat.indices.cpu().numpy()\n","                # sort the indices\n","                y_hat.sort()\n","                # print(f\"y_hat: {y_hat}\")\n","                # Compute the accuracy\n","                correct += np.sum(y == y_hat)\n","                total += len(y)\n","            avg_accuracy.append(correct/total*100)\n","    # Mean accuracy\n","    print(f\"Mean accuracy: {np.mean(avg_accuracy):.1f}%\")\n","    # Min accuracy\n","    print(f\"Min accuracy: {np.min(avg_accuracy):.1f}%\")\n","    # Number of samples with min accuracy\n","    print(f\"Number of samples with min accuracy: {np.sum(np.array(avg_accuracy) == np.min(avg_accuracy))}\")\n","    # Max accuracy\n","    print(f\"Max accuracy: {np.max(avg_accuracy):.1f}%\")\n","    # Number of samples with max accuracy\n","    print(f\"Number of samples with max accuracy: {np.sum(np.array(avg_accuracy) == np.max(avg_accuracy))}\")"]},{"cell_type":"code","execution_count":421,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T22:31:33.627847Z","iopub.status.busy":"2023-11-13T22:31:33.627155Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean accuracy: 30.6%\n","Min accuracy: 0.0%\n","Number of samples with min accuracy: 1673\n","Max accuracy: 100.0%\n","Number of samples with max accuracy: 418\n"]}],"source":["\n","test_predicted()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3998218,"sourceId":6960150,"sourceType":"datasetVersion"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
